{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv11 Implementation for Raspberry Pi\n",
    "\n",
    "This notebook implements a lightweight YOLOv11 model optimized for deployment on Raspberry Pi 5, using NCNN for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install torch torchvision numpy opencv-python ncnn\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ncnn\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Define the YOLOv11 model with a lightweight MobileNetV3-like backbone for efficient computation on Raspberry Pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        # Split predictions into box coordinates, objectness, and class predictions\n",
    "        pred_boxes = predictions[..., :4]\n",
    "        pred_obj = predictions[..., 4]\n",
    "        pred_cls = predictions[..., 5:]\n",
    "        \n",
    "        # Split targets similarly\n",
    "        target_boxes = targets[..., :4]\n",
    "        target_obj = targets[..., 4]\n",
    "        target_cls = targets[..., 5:]\n",
    "        \n",
    "        # Calculate losses\n",
    "        box_loss = self.mse(pred_boxes, target_boxes)\n",
    "        obj_loss = self.mse(pred_obj, target_obj)\n",
    "        cls_loss = self.mse(pred_cls, target_cls)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = box_loss + obj_loss + cls_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Conversion and Quantization\n",
    "\n",
    "Implement functions for converting the model to NCNN format and quantizing it for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ncnn(model, input_shape=(1, 3, 416, 416)):\n",
    "    \"\"\"Convert PyTorch model to NCNN format\"\"\"\n",
    "    # Export to ONNX first\n",
    "    dummy_input = torch.randn(input_shape)\n",
    "    torch.onnx.export(model, dummy_input, \"yolov11.onnx\",\n",
    "                     input_names=['input'],\n",
    "                     output_names=['output'],\n",
    "                     dynamic_axes={'input': {0: 'batch_size'},\n",
    "                                 'output': {0: 'batch_size'}})\n",
    "    \n",
    "    # Convert ONNX to NCNN\n",
    "    # Note: This requires the NCNN converter tool to be installed\n",
    "    !ncnn2table yolov11.onnx yolov11.table\n",
    "    !ncnn2param yolov11.onnx yolov11.param\n",
    "    !ncnn2bin yolov11.onnx yolov11.bin\n",
    "\n",
    "def quantize_model(model, calibration_data):\n",
    "    \"\"\"Quantize model for efficient inference\"\"\"\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "    torch.quantization.prepare(model, inplace=True)\n",
    "    \n",
    "    # Calibrate with sample data\n",
    "    with torch.no_grad():\n",
    "        for data in calibration_data:\n",
    "            model(data)\n",
    "    \n",
    "    torch.quantization.convert(model, inplace=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Train the model on your dataset (COCO or custom dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install COCO API if not already installed\n",
    "!pip install pycocotools\n",
    "\n",
    "# Import COCO dataset utilities\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "\n",
    "# Download COCO dataset\n",
    "def download_coco_dataset():\n",
    "    try:\n",
    "        print(\"Downloading COCO dataset...\")\n",
    "        \n",
    "        # Create directory for COCO dataset\n",
    "        os.makedirs('./coco', exist_ok=True)\n",
    "        \n",
    "        # Check if files already exist\n",
    "        if os.path.exists('./coco/val2017') and os.path.exists('./coco/annotations'):\n",
    "            print(\"COCO dataset already exists!\")\n",
    "            return\n",
    "        \n",
    "        # Download COCO 2017 validation set\n",
    "        !wget http://images.cocodataset.org/zips/val2017.zip\n",
    "        !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "        \n",
    "        print(\"Extracting files...\")\n",
    "        # Unzip the files\n",
    "        !unzip -q val2017.zip -d ./coco/\n",
    "        !unzip -q annotations_trainval2017.zip -d ./coco/\n",
    "        \n",
    "        print(\"Cleaning up zip files...\")\n",
    "        # Clean up zip files\n",
    "        !rm val2017.zip\n",
    "        !rm annotations_trainval2017.zip\n",
    "        \n",
    "        print(\"COCO dataset download complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading COCO dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "download_coco_dataset()\n",
    "\n",
    "# Setup COCO dataset\n",
    "def setup_coco_dataset():\n",
    "    # Create directory for COCO dataset\n",
    "    os.makedirs('./coco', exist_ok=True)\n",
    "    \n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((416, 416)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load COCO validation set (smaller than training set for faster processing)\n",
    "    dataset = CocoDetection(\n",
    "        root='./coco/val2017',\n",
    "        annFile='./coco/annotations/instances_val2017.json',\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Create data loader\n",
    "def create_data_loader(dataset, batch_size=8):\n",
    "    return data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "# Prepare calibration data\n",
    "def prepare_calibration_data(dataset, num_images=100):\n",
    "    calibration_data = []\n",
    "    for i in range(min(num_images, len(dataset))):\n",
    "        img, _ = dataset[i]\n",
    "        calibration_data.append(img.unsqueeze(0).to(device))\n",
    "    return calibration_data\n",
    "\n",
    "# Initialize model and dataset\n",
    "model = YOLOv11(num_classes=80).to(device)  # COCO has 80 classes\n",
    "dataset = setup_coco_dataset()\n",
    "data_loader = create_data_loader(dataset)\n",
    "\n",
    "# Prepare calibration data\n",
    "calibration_data = prepare_calibration_data(dataset)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, data_loader, num_epochs=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "    criterion = YOLOLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        # Add tqdm here for the data_loader loop\n",
    "        for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc=f'Epoch {epoch+1}/{num_epochs}')):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(data_loader)}, Loss: {loss.item():.4f}')\n",
    "            \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Update learning rate based on average loss\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        save_model(model, optimizer, epoch, avg_loss, f'yolov11_epoch_{epoch+1}.pth')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, data_loader)\n",
    "\n",
    "# Model evaluation\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = YOLOLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Evaluation Batch {batch_idx}/{len(data_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f'Evaluation Average Loss: {avg_loss:.4f}')\n",
    "    return avg_loss\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_loss = evaluate_model(model, data_loader)\n",
    "\n",
    "# Save the trained model\n",
    "def save_model(model, optimizer, epoch, loss, path='yolov11_trained.pth'):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, path)\n",
    "    print(f'Model saved to {path}')\n",
    "\n",
    "# Save the model after training\n",
    "save_model(model, optimizer, num_epochs, evaluation_loss)\n",
    "\n",
    "# Quantize model\n",
    "quantized_model = quantize_model(model, calibration_data)\n",
    "\n",
    "# Convert to NCNN\n",
    "convert_to_ncnn(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Create a detector class for running inference on the Raspberry Pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv11Detector:\n",
    "    def __init__(self, model_path, conf_threshold=0.25, nms_threshold=0.45):\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "        \n",
    "        # Load NCNN model\n",
    "        self.net = ncnn.Net()\n",
    "        self.net.load_param(\"yolov11.param\")\n",
    "        self.net.load_model(\"yolov11.bin\")\n",
    "        \n",
    "        # Initialize NCNN extractor\n",
    "        self.extractor = self.net.create_extractor()\n",
    "        self.extractor.set_num_threads(4)  # Adjust based on Raspberry Pi capabilities\n",
    "        \n",
    "    def preprocess(self, image):\n",
    "        \"\"\"Preprocess image for inference\"\"\"\n",
    "        # Resize to model input size\n",
    "        img = cv2.resize(image, (416, 416))\n",
    "        # Convert to RGB and normalize\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        # NCHW format\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        return img\n",
    "    \n",
    "    def detect(self, image):\n",
    "        \"\"\"Run inference and return detections\"\"\"\n",
    "        # Preprocess image\n",
    "        blob = self.preprocess(image)\n",
    "        \n",
    "        # Run inference\n",
    "        self.extractor.input(\"input\", blob)\n",
    "        output = self.extractor.extract(\"output\")\n",
    "        \n",
    "        # Process detections\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        class_ids = []\n",
    "        \n",
    "        # Convert output to detections\n",
    "        detections = output.reshape(-1, 85)  # 80 classes + 5 box coordinates\n",
    "        \n",
    "        # Filter by confidence\n",
    "        mask = detections[:, 4] > self.conf_threshold\n",
    "        detections = detections[mask]\n",
    "        \n",
    "        if len(detections) > 0:\n",
    "            # Convert to boxes\n",
    "            boxes = detections[:, :4]\n",
    "            scores = detections[:, 4]\n",
    "            class_ids = np.argmax(detections[:, 5:], axis=1)\n",
    "            \n",
    "            # Apply NMS\n",
    "            indices = cv2.dnn.NMSBoxes(boxes, scores, self.conf_threshold, self.nms_threshold)\n",
    "            \n",
    "            if len(indices) > 0:\n",
    "                boxes = boxes[indices]\n",
    "                scores = scores[indices]\n",
    "                class_ids = class_ids[indices]\n",
    "        \n",
    "        return boxes, scores, class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Raspberry Pi\n",
    "\n",
    "After converting the model to NCNN format, you can copy the following files to your Raspberry Pi:\n",
    "- yolov11.param\n",
    "- yolov11.bin\n",
    "Then use the YOLOv11Detector class to run inference on the Pi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
